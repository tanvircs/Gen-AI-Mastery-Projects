{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Tanvir Ahmed\\\\Desktop\\\\Gen-AI-Mastery-Projects\\\\End-to-End-Projects\\\\Source-Code-Analysis\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "repo = Repo.clone_from(\"https://github.com/tanvircs/Sentiment-Analysis-of-COVID-19-Vaccination-on-Twitter\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='def preprocess_word(word):\\n    word = word.lower()\\n    word = re.sub(\"\\'\", \"\", word)\\n    word = word.strip(\\'\\\\\\'\"?!,.():;\\')\\n    word = re.sub(r\\'(.)\\\\1+\\', r\\'\\\\1\\\\1\\', word)\\n    word = re.sub(\"@[A-Za-z0-9_]+\",\"\", word)\\n    word = re.sub(\"#[A-Za-z0-9_]+\",\"\", word)\\n    word = re.sub(r\\'http\\\\S+\\', \\'\\', word)\\n    word = re.sub(\"[^a-z0-9]\",\" \", word)\\n    word = re.sub(\\'\\\\[.*?\\\\]\\',\\' \\', word)\\n    word = re.sub(r\\'(-|\\\\\\')\\', \\'\\', word)\\n    return word', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 500,\n",
    "                                                             chunk_overlap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='def preprocess_word(word):\\n    word = word.lower()\\n    word = re.sub(\"\\'\", \"\", word)\\n    word = word.strip(\\'\\\\\\'\"?!,.():;\\')\\n    word = re.sub(r\\'(.)\\\\1+\\', r\\'\\\\1\\\\1\\', word)\\n    word = re.sub(\"@[A-Za-z0-9_]+\",\"\", word)\\n    word = re.sub(\"#[A-Za-z0-9_]+\",\"\", word)\\n    word = re.sub(r\\'http\\\\S+\\', \\'\\', word)\\n    word = re.sub(\"[^a-z0-9]\",\" \", word)\\n    word = re.sub(\\'\\\\[.*?\\\\]\\',\\' \\', word)\\n    word = re.sub(r\\'(-|\\\\\\')\\', \\'\\', word)\\n    return word', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"def handle_emojis(tweet):\\n    # Smile -- :), : ), :-), (:, ( :, (-:, :')\\n    tweet = re.sub(r'(:\\\\s?\\\\)|:-\\\\)|\\\\(\\\\s?:|\\\\(-:|:\\\\'\\\\))', ' EMO_POS ', tweet)\\n    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\\n    tweet = re.sub(r'(:\\\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\\n    # Love -- <3, :*\\n    tweet = re.sub(r'(<3|:\\\\*)', ' EMO_POS ', tweet)\\n    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\\n    tweet = re.sub(r'(;-?\\\\)|;-?D|\\\\(-?;)', ' EMO_POS ', tweet)\\n    # Sad -- :-(, : (, :(, ):, )-:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='tweet = re.sub(r\\'(:\\\\s?\\\\(|:-\\\\(|\\\\)\\\\s?:|\\\\)-:)\\', \\' EMO_NEG \\', tweet)\\n    # Cry -- :,(, :\\'(, :\"(\\n    tweet = re.sub(r\\'(:,\\\\(|:\\\\\\'\\\\(|:\"\\\\()\\', \\' EMO_NEG \\', tweet)\\n    return tweet', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def fix_bad_unicode(text, normalization=\"NFC\"):\\n    try:\\n        text = text.encode(\"latin\", \"backslashreplace\").decode(\"unicode-escape\")\\n    except:\\n        pass\\n\\n    return fix_text(text, normalization=normalization)', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"def remove_pattern(input_txt, pattern):\\n    r = re.findall(pattern, input_txt)\\n    for i in r:\\n        input_txt = re.sub(i, '', input_txt)\\n    return input_txt\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def hashtag_extract(x):\\n    hashtags = []    # Loop over the words in the tweet\\n    for i in x:\\n        ht = re.findall(r\"#(\\\\w+)\", i)\\n        hashtags.append(ht)\\n    return hashtags', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def word_vector(tokens, size):\\n    vec = np.zeros(size).reshape((1, size))\\n    count = 0\\n    for word in tokens:\\n        try:\\n            vec += model_w2v.wv[word].reshape((1, size))\\n            count += 1.\\n        except KeyError:  # handling the case where the token is not in vocabulary\\n            continue\\n    if count != 0:\\n        vec /= count\\n    return vec', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def add_label(twt):\\n    output = []\\n    for i, s in zip(twt.index, twt):\\n        output.append(TaggedDocument(s, [\"tweet_\" + str(i)]))\\n    return output', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='#!/usr/bin/env python\\n# coding: utf-8\\n\\n# In[1]:\\n\\n\\nimport re    # for regular expressions  \\nimport string \\nimport warnings \\nimport numpy as np \\nimport pandas as pd \\nimport seaborn as sns \\nimport matplotlib.pyplot as plt  \\n\\npd.set_option(\"display.max_colwidth\", 200) \\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \\n\\nget_ipython().magic(\\'matplotlib inline\\')\\n\\n\\n# In[2]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import json\\nimport csv\\nimport nltk\\nimport tweepy\\nfrom sklearn import metrics\\nimport re\\nimport os\\nfrom nltk.corpus import stopwords\\nfrom keras.layers import Dense, Dropout, Activation\\nfrom nltk.tokenize import word_tokenize\\nimport gensim\\nfrom gensim.models import Word2Vec\\nfrom wordcloud import WordCloud\\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \\nimport gensim\\nfrom tqdm import tqdm \\ntqdm.pandas(desc=\"progress-bar\")', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from gensim.models.doc2vec import TaggedDocument', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[3]:\\n\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import f1_score\\nfrom sklearn.metrics import classification_report\\nfrom xgboost import XGBClassifier', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.layers import Dense, Embedding, Dropout , Activation, Flatten, SimpleRNN\\nfrom keras.layers import GlobalMaxPool1D\\nfrom keras.models import Model, Sequential\\nimport tensorflow as tf', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from sklearn.svm import LinearSVC\\nfrom sklearn.svm import SVC\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.neural_network import MLPClassifier\\nfrom sklearn.gaussian_process import GaussianProcessClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\\nfrom sklearn.tree import DecisionTreeClassifier', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from sklearn.metrics import confusion_matrix, accuracy_score\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[4]:\\n\\n\\ndf1 = pd.read_csv('final_data.csv')\\n\\n\\n# In[5]:\\n\\n\\ndf1.head(0)\\n\\n\\n# In[6]:\\n\\n\\ndf1['split'] = np.random.randn(df1.shape[0], 1)\\n\\nmsk = np.random.rand(len(df1)) <= 0.7\\n\\ntrain_1 = df1[msk]\\ntest_1 = df1[~msk]\\n\\n\\n# In[7]:\\n\\n\\ntrain_1.to_csv('final_train.csv', encoding='utf-8')\\n\\n\\n# In[8]:\\n\\n\\ntest_1.to_csv('final_test.csv', encoding='utf-8')\\n\\n\\n# In[9]:\\n\\n\\ntrain  = pd.read_csv('final_train.csv') \\ntest = pd.read_csv('final_test.csv')\\n\\n\\n# In[10]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[10]:\\n\\n\\ntrain.drop(train.columns[[0,1]], axis=1, inplace=True)\\n\\n\\n# In[11]:\\n\\n\\ntest.drop(test.columns[[0,1]], axis=1, inplace=True)\\n\\n\\n# ## Exploring Data\\n\\n# In[12]:\\n\\n\\ntrain.info()\\n\\n\\n# In[13]:\\n\\n\\ntest.head()\\n\\n\\n# In[14]:\\n\\n\\ntrain.shape, test.shape\\n\\n\\n# In[15]:\\n\\n\\ntrain[\"sentiment\"].value_counts()\\n\\n\\n# In[16]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[16]:\\n\\n\\nplt.hist(train.text.str.len(), bins=20, label=\\'train\\')\\nplt.hist(test.text.str.len(), bins=20, label=\\'test\\')\\nplt.legend()\\nplt.title(\\'Tarin Test distribution in dataset\\', fontsize=20)\\nplt.xticks(fontsize=14)\\nplt.yticks(fontsize=14)\\n# plt.show()\\nplt.tight_layout()\\nplt.tight_layout()\\nplt.savefig(\"Train Test Distribution.png\", dpi=200, bbox_inches=\\'tight\\')\\n\\n\\n# In[17]:\\n\\n\\ncombi = train.append(test, ignore_index=True, sort=True)\\ncombi.shape\\n\\n\\n# In[18]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[18]:\\n\\n\\ncombi[\\'sentiment\\'].replace({1: 2, 0: 1, -1: 0}, inplace=True)\\n\\n\\n# In[19]:\\n\\n\\ncombi[\"sentiment\"].value_counts()\\n\\n\\n# In[20]:\\n\\n\\ncombi.drop(combi.columns[[6]], axis=1, inplace=True)\\n\\n\\n# In[21]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[21]:\\n\\n\\nprint(\"Number of rows per vaccine rating:\")\\nprint(combi[\\'sentiment\\'].value_counts())\\nplt.figure(figsize=(10, 7)) \\ncombi[\\'sentiment\\'].value_counts().sort_index().plot.bar()\\nplt.title(\\'Sentiment distribution in df\\', fontsize=20)\\nplt.xlabel(\\'Sentiment Label\\', fontsize=18)\\nplt.ylabel(\\'Tweet Count\\', fontsize=18)\\nplt.xticks(fontsize=14)\\nplt.yticks(fontsize=14)\\n# plt.show()\\nplt.tight_layout()\\nplt.savefig(\"Sentiment_Distribution.png\", dpi=200, bbox_inches=\\'tight\\')', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# ## Data Wrangling & Preprocessing\\n# ### Remove Punctuation: (a)hashtags (b)links (c)punctuations (d)non-alphanumeric characters\\n\\n# In[22]:\\n\\n\\n# Code for: def preprocess_word(word):\\n\\n\\n# In[23]:\\n\\n\\ncombi['tidy_tweet'] = combi['text'].apply(preprocess_word)\\ncombi.head()\\n\\n\\n# ## Stop word removal\\n\\n# In[24]:\\n\\n\\nstopwords = nltk.corpus.stopwords.words('english')\\ncombi['tidy_tweet'].apply(lambda x: [item for item in x if item not in stopwords])\\n\\n\\n# In[25]:\\n\\n\\ncombi.head()\\n\\n\\n# ## Handle Emojis\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[26]:\\n\\n\\n# Code for: def handle_emojis(tweet):\\n\\n\\n# In[27]:\\n\\n\\ncombi[\\'tidy_tweet\\'] = combi[\\'tidy_tweet\\'].apply(handle_emojis)\\ncombi.head()\\n\\n\\n# ## Fix Bad Unicode\\n\\n# In[28]:\\n\\n\\n# Code for: def fix_bad_unicode(text, normalization=\"NFC\"):\\n\\n\\n# In[29]:\\n\\n\\ncombi[\\'tidy_tweet\\'] = combi[\\'tidy_tweet\\'].apply(handle_emojis)\\ncombi.head()\\n\\n\\n# ## Remove Pattern\\n\\n# In[30]:\\n\\n\\n# Code for: def remove_pattern(input_txt, pattern):\\n\\n\\n# In[31]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[31]:\\n\\n\\ncombi[\\'tidy_tweet\\'] = np.vectorize(remove_pattern)(combi[\\'tidy_tweet\\'], \"@[\\\\w]*\") \\ncombi.head()\\n\\n\\n# ## Wordclouds\\n\\n# In[32]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[32]:\\n\\n\\nall_words = \\' \\'.join([text for text in combi[\\'tidy_tweet\\']]) \\n# fig = plt.figure(figsize=(10, 6))\\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words) \\nplt.figure(figsize=(10, 7)) \\nplt.imshow(wordcloud, interpolation=\"bilinear\") \\nplt.axis(\\'off\\')\\nplt.title(\\'Dataset Word Clouds\\', fontsize=20)\\n# plt.show()\\nplt.tight_layout()\\nplt.savefig(\"Word_Clouds.png\", dpi=200, bbox_inches=\\'tight\\')', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# normal_words =\\' \\'.join([text for text in combi[\\'tidy_tweet\\'][combi[\\'sentiment\\'] == 1]]) \\n\\n# wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\\n# plt.figure(figsize=(10, 7))\\n# plt.imshow(wordcloud, interpolation=\"bilinear\")\\n# plt.axis(\\'off\\')\\n# plt.title(\\'Neutral Word Clouds\\', fontsize=20)\\n# # plt.show()\\n# plt.tight_layout()\\n# plt.savefig(\"Neutral_Word_Clouds.png\", dpi=200, bbox_inches=\\'tight\\')\\n\\n\\n# ## Neutral Sentiment Words\\n\\n# In[33]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[33]:\\n\\n\\nnormal_words =\\' \\'.join([text for text in combi[\\'tidy_tweet\\'][combi[\\'sentiment\\'] == 1]]) \\n\\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\\nplt.figure(figsize=(10, 7))\\nplt.imshow(wordcloud, interpolation=\"bilinear\")\\nplt.axis(\\'off\\')\\nplt.title(\\'Neutral Word Clouds\\', fontsize=20)\\n# plt.show()\\nplt.tight_layout()\\nplt.savefig(\"Neutral_Word_Clouds.png\", dpi=200, bbox_inches=\\'tight\\')\\n\\n\\n# ## Positive Sentiment Words\\n\\n# In[34]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[34]:\\n\\n\\nnormal_words =\\' \\'.join([text for text in combi[\\'tidy_tweet\\'][combi[\\'sentiment\\'] == 2]]) \\n\\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\\nplt.figure(figsize=(10, 7))\\nplt.imshow(wordcloud, interpolation=\"bilinear\")\\nplt.axis(\\'off\\')\\nplt.title(\\'Positive Word Clouds\\', fontsize=20)\\n# plt.show()\\nplt.tight_layout()\\nplt.savefig(\"Positive_Word_Clouds.png\", dpi=200, bbox_inches=\\'tight\\')\\n\\n\\n# In[35]:\\n\\n\\ncombi[\\'sentiment\\'].value_counts()', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# ## Negative Sentiment Words\\n\\n# In[36]:\\n\\n\\nnormal_words =\\' \\'.join([text for text in combi[\\'tidy_tweet\\'][combi[\\'sentiment\\'] == 0]]) \\n\\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\\nplt.figure(figsize=(10, 7))\\nplt.imshow(wordcloud, interpolation=\"bilinear\")\\nplt.axis(\\'off\\')\\nplt.title(\\'Negative Word Clouds\\', fontsize=20)\\n# plt.show()\\nplt.tight_layout()\\nplt.savefig(\"Negative_Word_Clouds.png\", dpi=200, bbox_inches=\\'tight\\')', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# ## Impact on Hashtags\\n\\n# In[37]:\\n\\n\\n# Code for: def hashtag_extract(x):\\n\\n\\n# In[38]:\\n\\n\\nHT_neutral = hashtag_extract(combi['text'][combi['sentiment'] == 1])\\n\\n\\n# In[39]:\\n\\n\\nHT_positive = hashtag_extract(combi['text'][combi['sentiment'] == 2]) \\n\\n\\n# In[40]:\\n\\n\\nHT_negative = hashtag_extract(combi['text'][combi['sentiment'] == 0])\\n\\n\\n# In[41]:\\n\\n\\nHT_neutral = sum(HT_neutral,[]) \\nHT_positive = sum(HT_positive,[])\\nHT_negative = sum(HT_negative,[])\\n\\n\\n# In[42]:\\n\\n\\nlen(HT_negative)\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"len(HT_negative)\\n\\n\\n# ## Neutral Sentiment Hashtags\\n\\n# In[43]:\\n\\n\\na = nltk.FreqDist(HT_neutral)\\nd = pd.DataFrame(\\n    {\\n    'Hashtag': list(a.keys()),\\n    'Count': list(a.values())\\n    }\\n)\\n\\n\\n# In[44]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[44]:\\n\\n\\nd = d.nlargest(columns=\"Count\", n = 20)\\nplt.figure(figsize=(10,7))\\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\\nax.set(ylabel = \\'Count\\')\\nplt.xticks(rotation=90)\\nplt.title(\\'Neutral Words HashTags\\', fontsize=20)\\nplt.xticks(fontsize=14)\\nplt.yticks(fontsize=14)\\n# plt.show()\\nplt.tight_layout()\\nplt.savefig(\"Neutral_Word_Hashtags.png\", dpi=200, bbox_inches=\\'tight\\')\\n\\n\\n# ## Positive Sentiment Hashtags\\n\\n# In[45]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[45]:\\n\\n\\na = nltk.FreqDist(HT_positive)\\nd = pd.DataFrame(\\n    {\\n    'Hashtag': list(a.keys()),\\n    'Count': list(a.values())\\n    }\\n)\\n\\n\\n# In[46]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[46]:\\n\\n\\nd = d.nlargest(columns=\"Count\", n = 20)\\nplt.figure(figsize=(10,7))\\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\\nax.set(ylabel = \\'Count\\')\\nplt.xticks(rotation=90)\\nplt.title(\\'Positive Words HashTags\\', fontsize=20)\\nplt.xticks(fontsize=14)\\nplt.yticks(fontsize=14)\\n# plt.show()\\nplt.tight_layout()\\nplt.savefig(\"Positive_Word_Hashtags.png\", dpi=200, bbox_inches=\\'tight\\')\\n\\n\\n# ## Negative Sentiment Hashtags\\n\\n# In[47]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[47]:\\n\\n\\na = nltk.FreqDist(HT_negative)\\nd = pd.DataFrame(\\n    {\\n    \\'Hashtag\\': list(a.keys()),\\n    \\'Count\\': list(a.values())\\n    }\\n)\\n\\n\\n# In[48]:\\n\\n\\nd = d.nlargest(columns=\"Count\", n = 20)\\nplt.figure(figsize=(10,7))\\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\\nplt.xticks(rotation=90)\\nplt.title(\\'Negative Words HashTags\\', fontsize=20)\\nplt.xticks(fontsize=14)\\nplt.yticks(fontsize=14)\\n# plt.show()\\nplt.tight_layout()\\nplt.savefig(\"Negative_Word_Hashtags.png\", dpi=200, bbox_inches=\\'tight\\')', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# ## Bag-of-Words Features\\n\\n# ## Build a Classification Model\\n\\n# In[49]:\\n\\n\\nbow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\\nbow = bow_vectorizer.fit_transform(combi['tidy_tweet'])\\nbow.shape\\n\\n\\n# ## TF-IDF Features\\n\\n# In[50]:\\n\\n\\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\\ntfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])\\ntfidf.shape\\n\\n\\n# ## Word2Vec Features\\n\\n# In[51]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='get_ipython().run_cell_magic(\\'time\\', \\'\\', \"\\\\ntokenized_tweet = combi[\\'tidy_tweet\\'].apply(lambda x: x.split()) # tokenizing \\\\n\\\\nmodel_w2v = gensim.models.Word2Vec(\\\\n            tokenized_tweet,\\\\n            vector_size=500, # desired no. of features/independent variables\\\\n            window=5, # context window size\\\\n            min_count=2, # Ignores all words with total frequency lower than 2.                                  \\\\n            sg = 1, # 1 for skip-gram model\\\\n            hs = 0,\\\\n', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='hs = 0,\\\\n            negative = 10, # for negative sampling\\\\n            workers= 32, # no.of cores\\\\n            seed = 34\\\\n)\\\\n\\\\nmodel_w2v.train(tokenized_tweet, total_examples= len(combi[\\'tidy_tweet\\']), epochs=20)\")', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[52]:\\n\\n\\nmodel_w2v.wv.most_similar(positive=\"fever\")\\n\\n\\n# In[53]:\\n\\n\\nmodel_w2v.wv.most_similar(positive=\"throat\")\\n\\n\\n# In[54]:\\n\\n\\nmodel_w2v.wv[\\'moderna\\']\\n\\n\\n# In[55]:\\n\\n\\nlen(model_w2v.wv[\\'moderna\\'])\\n\\n\\n# ## Preparing Vectors for Tweets\\n\\n# In[56]:\\n\\n\\n# Code for: def word_vector(tokens, size):\\n\\n\\n# ## Preparing Word2Vec Feature Set\\n\\n# In[57]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[57]:\\n\\n\\nwordvec_arrays = np.zeros((len(tokenized_tweet), 500)) \\nfor i in range(len(tokenized_tweet)):\\n    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 500)\\nwordvec_df = pd.DataFrame(wordvec_arrays)\\nwordvec_df.shape\\n\\n\\n# ## Doc2Vec Embedding\\n\\n# In[58]:\\n\\n\\n# Code for: def add_label(twt):\\n\\nlabeled_tweets = add_label(tokenized_tweet) # label all the tweets\\n\\n\\n# In[59]:\\n\\n\\nlabeled_tweets[:6]\\n\\n\\n# In[60]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='get_ipython().run_cell_magic(\\'time\\', \\'\\', \"model_d2v = gensim.models.Doc2Vec(dm=1, # dm = 1 for ‘distributed memory’ model\\\\n                                  dm_mean=1, # dm_mean = 1 for using mean of the context word vectors\\\\n                                  vector_size=500, # no. of desired features\\\\n                                  window=5, # width of the context window                                  \\\\n                                  negative=7, # if > 0 then negative sampling will be', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='sampling will be used\\\\n                                  min_count=5, # Ignores all words with total frequency lower than 5.                                  \\\\n                                  workers=32, # no. of cores                                  \\\\n                                  alpha=0.1, # learning rate                                  \\\\n                                  seed = 23, # for reproducibility\\\\n                                 ) \\\\n\\\\nmodel_d2v.build_vocab([i for i in', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='for i in tqdm(labeled_tweets)])\\\\n\\\\nmodel_d2v.train(labeled_tweets, total_examples= len(combi[\\'tidy_tweet\\']), epochs=15)\")', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[61]:\\n\\n\\ndocvec_arrays = np.zeros((len(tokenized_tweet), 500)) \\nfor i in range(len(combi)):\\n    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,500))    \\n\\ndocvec_df = pd.DataFrame(docvec_arrays) \\ndocvec_df.shape\\n\\n\\n# In[62]:\\n\\n\\n# Extracting train and test BoW features \\ntrain_bow = bow[:16180,:]\\n\\n# splitting data into training and validation set \\nx_train,x_test,y_train,y_test = train_test_split(train_bow, combi['sentiment'], random_state=42, test_size=0.3)\\n\\n\\n# In[63]:\\n\\n\\ntype(train_bow)\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"type(train_bow)\\n\\n\\n# ## Machine Learning for CounterVectorization\\n\\n# ## SVC\\n\\n# In[64]:\\n\\n\\nsvc_model = LinearSVC(class_weight='balanced',C=1, penalty='l2', max_iter=1500,loss='squared_hinge',\\n                        multi_class='ovr').fit(x_train, y_train)\\n\\nsvc_model_predict = svc_model.predict(x_test)\\nsvc_report = classification_report(y_test, svc_model_predict )\\n\\nprint(svc_report)\\n\\n\\n# ## SGDClassifier\\n\\n# In[65]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[65]:\\n\\n\\nsgd_model = SGDClassifier(n_jobs=-1,class_weight='balanced',penalty='l2').fit(x_train, y_train)\\nsgd_model_predict = sgd_model.predict(x_test)\\nsgd_report = classification_report(y_test, sgd_model_predict )\\n\\nprint(sgd_report)\\n\\n\\n# ## MLPClassifier\\n\\n# In[66]:\\n\\n\\nmlp = MLPClassifier().fit(x_train, y_train)\\nmlp_predict = mlp.predict(x_test)\\nmlp_report = classification_report(y_test, mlp_predict )\\nprint(mlp_report)\\n\\n\\n# ## KNeighborsClassifier\\n\\n# In[67]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[67]:\\n\\n\\nclassifier = KNeighborsClassifier(n_neighbors=5,algorithm='brute') \\nclassifier.fit(x_train, y_train) \\npredicted_label = classifier.predict(x_test) \\nknn_model_report = classification_report(y_test, predicted_label)\\nprint(knn_model_report)\\n\\n\\n# ## RandomForestClassifier\\n\\n# In[68]:\\n\\n\\nrf = RandomForestClassifier(n_estimators=100).fit(x_train, y_train)\\nrf_predict = rf.predict(x_test)\\nrf_report = classification_report(y_test, rf_predict )\\nprint(rf_report)\\n\\n\\n# ## AdaBoostClassifier\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[69]:\\n\\n\\nab = AdaBoostClassifier(n_estimators=100, learning_rate=1).fit(x_train, y_train)\\nab_predict = ab.predict(x_test)\\nab_report = classification_report(y_test, ab_predict )\\nprint(ab_report)\\n\\n\\n# ## BaggingClassifier\\n\\n# In[70]:\\n\\n\\nbagging = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=0).fit(x_train, y_train)\\nbagging_predict = bagging.predict(x_test)\\nbagging_report = classification_report(y_test, bagging_predict )\\nprint(bagging_report)\\n\\n\\n# ## ExtraTreesClassifier', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[71]:\\n\\n\\net = ExtraTreesClassifier(n_estimators=100, random_state=0).fit(x_train, y_train)\\net_predict = et.predict(x_test)\\net_report = classification_report(y_test, et_predict)\\nprint(et_report)\\n\\n\\n# ## DecisionTreeClassifier\\n\\n# In[72]:\\n\\n\\nclf = DecisionTreeClassifier(criterion=\"entropy\")\\nclf = clf.fit(x_train,y_train)\\ny_pred = clf.predict(x_test)\\ndt_model_report = classification_report(y_test, y_pred)\\nprint(dt_model_report)\\n\\n\\n# ## Logistic Regression\\n\\n# In[73]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[73]:\\n\\n\\nlogistic_reg_model = LogisticRegression(n_jobs = -1, penalty='l2', multi_class='multinomial',class_weight = 'balanced',verbose=1).fit(x_train,y_train)\\n\\nlr_model_predict = logistic_reg_model.predict(x_test)\\nlr_model_report = classification_report(y_test, lr_model_predict)\\n\\nprint(lr_model_report)\\n\\n\\n# ## Padding and Tokenization\\n\\n# In[74]:\\n\\n\\nx = combi.tidy_tweet\\ny = combi['sentiment']\\n\\n\\n# In[75]:\\n\\n\\nprint(x.shape)\\nprint(y.shape)\\n\\n\\n# In[76]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[76]:\\n\\n\\nnum_words = 8000\\nembed_dim = 32\\ntokenizer = Tokenizer(num_words=num_words,oov_token = \"<oov>\" )\\ntokenizer.fit_on_texts(x)\\nword_index=tokenizer.word_index\\nsequences = tokenizer.texts_to_sequences(x)\\nlength=[]\\nfor i in sequences:\\n    length.append(len(i))\\nprint(len(length))\\nprint(\"Mean is: \",np.mean(length))\\nprint(\"Max is: \",np.max(length))\\nprint(\"Min is: \",np.min(length))\\n\\n\\n# In[77]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[77]:\\n\\n\\npad_length = 24\\nsequences = pad_sequences(sequences, maxlen = pad_length, truncating = 'pre', padding = 'post')\\nsequences.shape\\n\\n\\n# In[78]:\\n\\n\\nx_train,x_test,y_train,y_test = train_test_split(sequences,y,test_size = 0.05)\\nprint(x_train.shape)\\nprint(x_test.shape)\\nprint(y_train.shape)\\nprint(y_test.shape)\\n\\n\\n# ## Training Deep Learning Model and Embeddings\\n\\n# In[79]:\\n\\n\\nrecall = tf.keras.metrics.Recall()\\nprecision = tf.keras.metrics.Precision()\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"model = Sequential([Embedding(num_words, embed_dim, input_length = pad_length),\\n                   SimpleRNN(8, return_sequences = True),\\n                   GlobalMaxPool1D(),\\n                   Dense(20,activation = 'relu',kernel_initializer='he_uniform'),\\n                   Dropout(0.25),\\n                   Dense(3,activation = 'softmax')])\\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\nmodel.name = 'Twitter Hate Text Classification'\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='model.summary()', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[80]:\\n\\n\\nhistory = model.fit(x = x_train, y = y_train, epochs = 5,validation_split = 0.05)\\n\\n\\n# In[81]:\\n\\n\\ntrain_acc = history.history['accuracy']\\nvalid_acc = history.history['val_accuracy']\\ntrain_loss = history.history['loss']\\nval_loss = history.history['val_loss']\\n\\n\\n# In[82]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[82]:\\n\\n\\nfig=plt.figure(figsize=(10,10))\\nfig.add_subplot(2, 1, 1)\\nplt.grid()\\nplt.plot(train_loss, color=\\'blue\\', label=\\'Train\\')\\nplt.plot(val_loss, color=\\'red\\', label=\\'Validation\\')\\nplt.legend()\\nplt.title(\\'Loss Counter Vectorizer\\', fontsize=20)\\nplt.xticks(fontsize=14)\\nplt.yticks(fontsize=14)\\nplt.tight_layout()\\nfig.savefig(\"Loss_CounterVectorizer.png\", dpi=200, bbox_inches=\\'tight\\')', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='fig.add_subplot(2, 1, 2)\\nplt.grid()\\nplt.plot(train_acc, color=\\'blue\\', label=\\'Train\\')\\nplt.plot(valid_acc, color=\\'red\\', label=\\'Validation\\')\\nplt.legend()\\nplt.title(\\'Classification Accuracy Counter Vectorizer\\', fontsize=20)\\nplt.xticks(fontsize=14)\\nplt.yticks(fontsize=14)\\nplt.tight_layout()\\nfig.savefig(\"Classifier_Accuracy_CounterVectorizer.png\", dpi=200, bbox_inches=\\'tight\\')\\n\\n\\n# In[83]:\\n\\n\\nevaluate = model.evaluate(x_test,y_test)\\n\\n\\n# In[84]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[84]:\\n\\n\\nprint(\"Test Acuracy is : {:.2f} %\".format(evaluate[1]*100))\\nprint(\"Test Loss is : {:.4f}\".format(evaluate[0]))\\n\\n\\n# In[85]:\\n\\n\\npredictions = model.predict(x_test)\\n\\n\\n# In[86]:\\n\\n\\npredict = []\\nfor i in predictions:\\n    predict.append(np.argmax(i))\\n\\n\\n# In[87]:\\n\\n\\ncm = confusion_matrix(predict,y_test)\\nacc = accuracy_score(predict,y_test)\\n\\n\\n# In[88]:\\n\\n\\nprint(\"The Confusion matrix is: \\\\n\",cm)\\n\\n\\n# In[89]:\\n\\n\\nprint(acc*100)\\n\\n\\n# In[90]:\\n\\n\\nprint(metrics.classification_report(y_test, predict))', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# # Machine Learning for TF_IDF\\n# ## TF-IDF Features\\n\\n# In[91]:\\n\\n\\ntrain_tfidf = tfidf[:16180,:]\\n\\nx_train,x_test,y_train,y_test = train_test_split(train_tfidf, combi['sentiment'], random_state=42, test_size=0.3) \\n\\nxtrain_tfidf = train_tfidf[y_train.index]\\nxvalid_tfidf = train_tfidf[y_test.index]\\n\\n\\n# ## SVC\\n\\n# In[92]:\\n\\n\\nsvc_model = LinearSVC(class_weight='balanced',C=1, penalty='l2', max_iter=1500,loss='squared_hinge',\\n                        multi_class='ovr').fit(x_train, y_train)\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"svc_model_predict = svc_model.predict(x_test)\\nsvc_report = classification_report(y_test, svc_model_predict )\\n\\nprint(svc_report)\\n\\n\\n# ## SGD Classifier\\n\\n# In[93]:\\n\\n\\nsgd_model = SGDClassifier(n_jobs=-1,class_weight='balanced',penalty='l2').fit(x_train, y_train)\\nsgd_model_predict = sgd_model.predict(x_test)\\nsgd_report = classification_report(y_test, sgd_model_predict )\\n\\nprint(sgd_report)\\n\\n\\n# ## MLP Classifier\\n\\n# In[94]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[94]:\\n\\n\\nmlp = MLPClassifier().fit(x_train, y_train)\\nmlp_predict = mlp.predict(x_test)\\nmlp_report = classification_report(y_test, mlp_predict )\\nprint(mlp_report)\\n\\n\\n# ## KNeighbors Classifier\\n\\n# In[95]:\\n\\n\\nclassifier = KNeighborsClassifier(n_neighbors=5,algorithm='brute') \\nclassifier.fit(x_train, y_train) \\npredicted_label = classifier.predict(x_test) \\nknn_model_report = classification_report(y_test, predicted_label)\\nprint(knn_model_report)\\n\\n\\n# ## Random Forest Classifier\\n\\n# In[96]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[96]:\\n\\n\\nrf = RandomForestClassifier(n_estimators=100).fit(x_train, y_train)\\nrf_predict = rf.predict(x_test)\\nrf_report = classification_report(y_test, rf_predict )\\nprint(rf_report)\\n\\n\\n# ## AdaBoost Classifier\\n\\n# In[97]:\\n\\n\\nab = AdaBoostClassifier(n_estimators=100, learning_rate=1).fit(x_train, y_train)\\nab_predict = ab.predict(x_test)\\nab_report = classification_report(y_test, ab_predict )\\nprint(ab_report)\\n\\n\\n# ## Bagging Classifier\\n\\n# In[98]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[98]:\\n\\n\\nbagging = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=0).fit(x_train, y_train)\\nbagging_predict = bagging.predict(x_test)\\nbagging_report = classification_report(y_test, bagging_predict )\\nprint(bagging_report)\\n\\n\\n# ## ExtraTrees Classifier\\n\\n# In[99]:\\n\\n\\net = ExtraTreesClassifier(n_estimators=100, random_state=0).fit(x_train, y_train)\\net_predict = et.predict(x_test)\\net_report = classification_report(y_test, et_predict)\\nprint(et_report)', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# ## DecisionTree Classifier\\n\\n# In[100]:\\n\\n\\nclf = DecisionTreeClassifier(criterion=\"entropy\")\\nclf = clf.fit(x_train,y_train)\\ny_pred = clf.predict(x_test)\\ndt_model_report = classification_report(y_test, y_pred)\\nprint(dt_model_report)\\n\\n\\n# ## Logistic Regression\\n\\n# In[101]:\\n\\n\\nlogistic_reg_model = LogisticRegression(n_jobs = -1, penalty=\\'l2\\', multi_class=\\'multinomial\\',class_weight = \\'balanced\\',verbose=1).fit(x_train,y_train)', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='lr_model_predict = logistic_reg_model.predict(x_test)\\nlr_model_report = classification_report(y_test, lr_model_predict)\\n\\nprint(lr_model_report)\\n\\n\\n# ## Padding and Tokenizing for TF-IDF\\n\\n# In[102]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[102]:\\n\\n\\nnum_words = 8000\\nembed_dim = 32\\ntokenizer = Tokenizer(num_words=num_words,oov_token = \"<oov>\" )\\ntokenizer.fit_on_texts(x)\\nword_index=tokenizer.word_index\\nsequences = tokenizer.texts_to_sequences(x)\\nlength=[]\\nfor i in sequences:\\n    length.append(len(i))\\nprint(len(length))\\nprint(\"Mean is: \",np.mean(length))\\nprint(\"Max is: \",np.max(length))\\nprint(\"Min is: \",np.min(length))\\n\\n\\n# In[103]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[103]:\\n\\n\\npad_length = 24\\nsequences = pad_sequences(sequences, maxlen = pad_length, truncating = 'pre', padding = 'post')\\nsequences.shape\\n\\n\\n# In[104]:\\n\\n\\nx_train,x_test,y_train,y_test = train_test_split(sequences,y,test_size = 0.05)\\nprint(x_train.shape)\\nprint(x_test.shape)\\nprint(y_train.shape)\\nprint(y_test.shape)\\n\\n\\n# ## Training Deep Learning Model on Embedding for TF-IDF\\n\\n# In[105]:\\n\\n\\nrecall = tf.keras.metrics.Recall()\\nprecision = tf.keras.metrics.Precision()\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"model = Sequential([Embedding(num_words, embed_dim, input_length = pad_length),\\n                   SimpleRNN(8, return_sequences = True),\\n                   GlobalMaxPool1D(),\\n                   Dense(20,activation = 'relu',kernel_initializer='he_uniform'),\\n                   Dropout(0.25),\\n                   Dense(3,activation = 'softmax')])\\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\nmodel.name = 'Twitter Hate Text Classification'\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='model.summary()', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[106]:\\n\\n\\nhistory = model.fit(x = x_train, y = y_train, epochs = 5,validation_split = 0.05)\\n\\n\\n# In[107]:\\n\\n\\ntrain_acc = history.history['accuracy']\\nvalid_acc = history.history['val_accuracy']\\ntrain_loss = history.history['loss']\\nval_loss = history.history['val_loss']\\n\\n\\n# In[108]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[108]:\\n\\n\\nfig=plt.figure(figsize=(10,10))\\nfig.add_subplot(2, 1, 1)\\nplt.grid()\\nplt.plot(train_loss, color=\\'blue\\', label=\\'Train\\')\\nplt.plot(val_loss, color=\\'red\\', label=\\'Validation\\')\\nplt.legend()\\nplt.title(\\'Loss TF-IDF\\', fontsize=20)\\nplt.xticks(fontsize=14)\\nplt.yticks(fontsize=14)\\nplt.tight_layout()\\nfig.savefig(\"Loss_TF_IDF.png\", dpi=200, bbox_inches=\\'tight\\')', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='fig.add_subplot(2, 1, 2)\\nplt.grid()\\nplt.plot(train_acc, color=\\'blue\\', label=\\'Train\\')\\nplt.plot(valid_acc, color=\\'red\\', label=\\'Validation\\')\\nplt.legend()\\nplt.title(\\'Classification Accuracy TF-IDF\\', fontsize=20)\\nplt.xticks(fontsize=14)\\nplt.yticks(fontsize=14)\\nplt.tight_layout()\\nfig.savefig(\"Classifier_Accuracy_TF_IDF.png\", dpi=200, bbox_inches=\\'tight\\')\\n\\n\\n# In[109]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[109]:\\n\\n\\nevaluate = model.evaluate(x_test,y_test)\\nprint(\"Test Acuracy is : {:.2f} %\".format(evaluate[1]*100))\\nprint(\"Test Loss is : {:.4f}\".format(evaluate[0]))\\npredictions = model.predict(x_test)\\npredict = []\\nfor i in predictions:\\n    predict.append(np.argmax(i))\\n    \\ncm = confusion_matrix(predict,y_test)\\nacc = accuracy_score(predict,y_test)\\n\\nprint(\"The Confusion matrix is: \\\\n\",cm)\\nprint(acc*100)\\nprint(metrics.classification_report(y_test, predict))', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# # Machine learning for Word2Vec\\n# ## Word2vec Features\\n\\n# In[110]:\\n\\n\\ntrain_w2v = wordvec_df.iloc[:16180,:]\\nx_train,x_test,y_train,y_test = train_test_split(train_w2v, combi['sentiment'], random_state=42, test_size=0.3)\\nxtrain_w2v = train_w2v.iloc[y_train.index,:]\\nxvalid_w2v = train_w2v.iloc[y_test.index,:]\\n\\n\\n# ## SVC\\n\\n# In[111]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[111]:\\n\\n\\nsvc_model = LinearSVC(class_weight='balanced',C=1, penalty='l2', max_iter=1500,loss='squared_hinge',\\n                        multi_class='ovr').fit(x_train, y_train)\\n\\nsvc_model_predict = svc_model.predict(x_test)\\nsvc_report = classification_report(y_test, svc_model_predict )\\n\\nprint(svc_report)\\n\\n\\n# ## SGD Classifier\\n\\n# In[112]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[112]:\\n\\n\\nsgd_model = SGDClassifier(n_jobs=-1,class_weight='balanced',penalty='l2').fit(x_train, y_train)\\nsgd_model_predict = sgd_model.predict(x_test)\\nsgd_report = classification_report(y_test, sgd_model_predict )\\n\\nprint(sgd_report)\\n\\n\\n# ## MLP Classifier\\n\\n# In[113]:\\n\\n\\nmlp = MLPClassifier().fit(x_train, y_train)\\nmlp_predict = mlp.predict(x_test)\\nmlp_report = classification_report(y_test, mlp_predict )\\nprint(mlp_report)\\n\\n\\n# ## KNeighbors Classifier\\n\\n# In[114]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[114]:\\n\\n\\nclassifier = KNeighborsClassifier(n_neighbors=5,algorithm='brute') \\nclassifier.fit(x_train, y_train) \\npredicted_label = classifier.predict(x_test) \\nknn_model_report = classification_report(y_test, predicted_label)\\nprint(knn_model_report)\\n\\n\\n# ## RandomForest Classifier\\n\\n# In[115]:\\n\\n\\nrf = RandomForestClassifier(n_estimators=100).fit(x_train, y_train)\\nrf_predict = rf.predict(x_test)\\nrf_report = classification_report(y_test, rf_predict )\\nprint(rf_report)\\n\\n\\n# ## AdaBoost Classifier\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[116]:\\n\\n\\nab = AdaBoostClassifier(n_estimators=100, learning_rate=1).fit(x_train, y_train)\\nab_predict = ab.predict(x_test)\\nab_report = classification_report(y_test, ab_predict )\\nprint(ab_report)\\n\\n\\n# ## Bagging Classifier\\n\\n# In[117]:\\n\\n\\nbagging = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=0).fit(x_train, y_train)\\nbagging_predict = bagging.predict(x_test)\\nbagging_report = classification_report(y_test, bagging_predict )\\nprint(bagging_report)', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# ## ExtraTrees Classifier\\n\\n# In[118]:\\n\\n\\net = ExtraTreesClassifier(n_estimators=100, random_state=0).fit(x_train, y_train)\\net_predict = et.predict(x_test)\\net_report = classification_report(y_test, et_predict)\\nprint(et_report)\\n\\n\\n# ## DecisionTree Classifier\\n\\n# In[119]:\\n\\n\\nclf = DecisionTreeClassifier(criterion=\"entropy\")\\nclf = clf.fit(x_train,y_train)\\ny_pred = clf.predict(x_test)\\ndt_model_report = classification_report(y_test, y_pred)\\nprint(dt_model_report)\\n\\n\\n# ## Logistic Regression', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[120]:\\n\\n\\nlogistic_reg_model = LogisticRegression(n_jobs = -1, penalty='l2', multi_class='multinomial',class_weight = 'balanced',verbose=1).fit(x_train,y_train)\\n\\nlr_model_predict = logistic_reg_model.predict(x_test)\\nlr_model_report = classification_report(y_test, lr_model_predict)\\n\\nprint(lr_model_report)\\n\\n\\n# ## Padding and Tokenization for Word2Vec\\n\\n# In[121]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[121]:\\n\\n\\nnum_words = 8000\\nembed_dim = 32\\ntokenizer = Tokenizer(num_words=num_words,oov_token = \"<oov>\" )\\ntokenizer.fit_on_texts(x)\\nword_index=tokenizer.word_index\\nsequences = tokenizer.texts_to_sequences(x)\\nlength=[]\\nfor i in sequences:\\n    length.append(len(i))\\nprint(len(length))\\nprint(\"Mean is: \",np.mean(length))\\nprint(\"Max is: \",np.max(length))\\nprint(\"Min is: \",np.min(length))\\n\\n\\n# In[122]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[122]:\\n\\n\\npad_length = 24\\nsequences = pad_sequences(sequences, maxlen = pad_length, truncating = 'pre', padding = 'post')\\nsequences.shape\\n\\nx_train,x_test,y_train,y_test = train_test_split(sequences,y,test_size = 0.05)\\nprint(x_train.shape)\\nprint(x_test.shape)\\nprint(y_train.shape)\\nprint(y_test.shape)\\n\\n\\n# ## Training Deep Learning Model on Embeddings for Word2vec\\n\\n# In[123]:\\n\\n\\nrecall = tf.keras.metrics.Recall()\\nprecision = tf.keras.metrics.Precision()\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"model = Sequential([Embedding(num_words, embed_dim, input_length = pad_length),\\n                   SimpleRNN(8, return_sequences = True),\\n                   GlobalMaxPool1D(),\\n                   Dense(20,activation = 'relu',kernel_initializer='he_uniform'),\\n                   Dropout(0.25),\\n                   Dense(3,activation = 'softmax')])\\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\nmodel.name = 'Twitter Hate Text Classification'\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='model.summary()', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[124]:\\n\\n\\nhistory = model.fit(x = x_train, y = y_train, epochs = 5,validation_split = 0.05)\\n\\ntrain_acc = history.history['accuracy']\\nvalid_acc = history.history['val_accuracy']\\ntrain_loss = history.history['loss']\\nval_loss = history.history['val_loss']\\n\\n\\n# In[125]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[125]:\\n\\n\\nfig=plt.figure(figsize=(10,10))\\nfig.add_subplot(2, 1, 1)\\nplt.grid()\\nplt.plot(train_loss, color=\\'blue\\', label=\\'Train\\')\\nplt.plot(val_loss, color=\\'red\\', label=\\'Validation\\')\\nplt.legend()\\nplt.title(\\'Loss Word2Vec\\', fontsize=20)\\nplt.xticks(fontsize=14)\\nplt.yticks(fontsize=14)\\nplt.tight_layout()\\nfig.savefig(\"Loss_Word2Vec.png\", dpi=200, bbox_inches=\\'tight\\')', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='fig.add_subplot(2, 1, 2)\\nplt.grid()\\nplt.plot(train_acc, color=\\'blue\\', label=\\'Train\\')\\nplt.plot(valid_acc, color=\\'red\\', label=\\'Validation\\')\\nplt.legend()\\nplt.title(\\'Classification Accuracy Word2Vec\\', fontsize=20)\\nplt.xticks(fontsize=14)\\nplt.yticks(fontsize=14)\\nplt.tight_layout()\\nfig.savefig(\"Classifier_Accuracy_Word2Vec.png\", dpi=200, bbox_inches=\\'tight\\')\\n\\n\\n# In[126]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[126]:\\n\\n\\nevaluate = model.evaluate(x_test,y_test)\\nprint(\"Test Acuracy is : {:.2f} %\".format(evaluate[1]*100))\\nprint(\"Test Loss is : {:.4f}\".format(evaluate[0]))\\npredictions = model.predict(x_test)\\npredict = []\\nfor i in predictions:\\n    predict.append(np.argmax(i))\\n    \\ncm = confusion_matrix(predict,y_test)\\nacc = accuracy_score(predict,y_test)\\n\\nprint(\"The Confusion matrix is: \\\\n\",cm)\\nprint(acc*100)\\nprint(metrics.classification_report(y_test, predict))', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# # Machine Learning for Doc2Vec\\n# ## Doc2Vec Features\\n\\n# In[127]:\\n\\n\\ntrain_d2v = docvec_df.iloc[:16180,:]\\nx_train,x_test,y_train,y_test = train_test_split(train_d2v, combi['sentiment'], random_state=42, test_size=0.3)\\nxtrain_d2v = train_d2v.iloc[y_train.index,:]\\nxvalid_d2v = train_d2v.iloc[y_test.index,:]\\n\\n\\n# ## SVC\\n\\n# In[128]:\\n\\n\\nsvc_model = LinearSVC(class_weight='balanced',C=1, penalty='l2', max_iter=1500,loss='squared_hinge',\\n                        multi_class='ovr').fit(x_train, y_train)\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"svc_model_predict = svc_model.predict(x_test)\\nsvc_report = classification_report(y_test, svc_model_predict )\\n\\nprint(svc_report)\\n\\n\\n# ## SGD Classifier\\n\\n# In[129]:\\n\\n\\nsgd_model = SGDClassifier(n_jobs=-1,class_weight='balanced',penalty='l2').fit(x_train, y_train)\\nsgd_model_predict = sgd_model.predict(x_test)\\nsgd_report = classification_report(y_test, sgd_model_predict )\\n\\nprint(sgd_report)\\n\\n\\n# ## MLP Classifier\\n\\n# In[130]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[130]:\\n\\n\\nmlp = MLPClassifier().fit(x_train, y_train)\\nmlp_predict = mlp.predict(x_test)\\nmlp_report = classification_report(y_test, mlp_predict )\\nprint(mlp_report)\\n\\n\\n# ## KNeighbors Classifier\\n\\n# In[131]:\\n\\n\\nclassifier = KNeighborsClassifier(n_neighbors=5,algorithm='brute') \\nclassifier.fit(x_train, y_train) \\npredicted_label = classifier.predict(x_test) \\nknn_model_report = classification_report(y_test, predicted_label)\\nprint(knn_model_report)\\n\\n\\n# ## RandomForest Classifier\\n\\n# In[132]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[132]:\\n\\n\\nrf = RandomForestClassifier(n_estimators=100).fit(x_train, y_train)\\nrf_predict = rf.predict(x_test)\\nrf_report = classification_report(y_test, rf_predict )\\nprint(rf_report)\\n\\n\\n# ## AdaBoost Classifier\\n\\n# In[133]:\\n\\n\\nab = AdaBoostClassifier(n_estimators=100, learning_rate=1).fit(x_train, y_train)\\nab_predict = ab.predict(x_test)\\nab_report = classification_report(y_test, ab_predict )\\nprint(ab_report)\\n\\n\\n# ## Bagging Classifier\\n\\n# In[134]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[134]:\\n\\n\\nbagging = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=0).fit(x_train, y_train)\\nbagging_predict = bagging.predict(x_test)\\nbagging_report = classification_report(y_test, bagging_predict )\\nprint(bagging_report)\\n\\n\\n# ## ExtraTrees Classifier\\n\\n# In[135]:\\n\\n\\net = ExtraTreesClassifier(n_estimators=100, random_state=0).fit(x_train, y_train)\\net_predict = et.predict(x_test)\\net_report = classification_report(y_test, et_predict)\\nprint(et_report)', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# ## DecisionTree Classifier\\n\\n# In[136]:\\n\\n\\nclf = DecisionTreeClassifier(criterion=\"entropy\")\\nclf = clf.fit(x_train,y_train)\\ny_pred = clf.predict(x_test)\\ndt_model_report = classification_report(y_test, y_pred)\\nprint(dt_model_report)\\n\\n\\n# ## Logistic Regression\\n\\n# In[137]:\\n\\n\\nlogistic_reg_model = LogisticRegression(n_jobs = -1, penalty=\\'l2\\', multi_class=\\'multinomial\\',class_weight = \\'balanced\\',verbose=1).fit(x_train,y_train)', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='lr_model_predict = logistic_reg_model.predict(x_test)\\nlr_model_report = classification_report(y_test, lr_model_predict)\\n\\nprint(lr_model_report)\\n\\n\\n# ## Padding and Tokenzation Doc2Feature\\n\\n# In[138]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[138]:\\n\\n\\nnum_words = 8000\\nembed_dim = 32\\ntokenizer = Tokenizer(num_words=num_words,oov_token = \"<oov>\" )\\ntokenizer.fit_on_texts(x)\\nword_index=tokenizer.word_index\\nsequences = tokenizer.texts_to_sequences(x)\\nlength=[]\\nfor i in sequences:\\n    length.append(len(i))\\nprint(len(length))\\nprint(\"Mean is: \",np.mean(length))\\nprint(\"Max is: \",np.max(length))\\nprint(\"Min is: \",np.min(length))', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"pad_length = 24\\nsequences = pad_sequences(sequences, maxlen = pad_length, truncating = 'pre', padding = 'post')\\nsequences.shape\\n\\nx_train,x_test,y_train,y_test = train_test_split(sequences,y,test_size = 0.05)\\nprint(x_train.shape)\\nprint(x_test.shape)\\nprint(y_train.shape)\\nprint(y_test.shape)\\n\\n\\n# ## Training Deep Learning Model on Embeddings Doc2Feature\\n\\n# In[139]:\\n\\n\\nrecall = tf.keras.metrics.Recall()\\nprecision = tf.keras.metrics.Precision()\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"model = Sequential([Embedding(num_words, embed_dim, input_length = pad_length),\\n                   SimpleRNN(8, return_sequences = True),\\n                   GlobalMaxPool1D(),\\n                   Dense(20,activation = 'relu',kernel_initializer='he_uniform'),\\n                   Dropout(0.25),\\n                   Dense(3,activation = 'softmax')])\\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\nmodel.name = 'Twitter Hate Text Classification'\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='model.summary()', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"# In[140]:\\n\\n\\nhistory = model.fit(x = x_train, y = y_train, epochs = 5,validation_split = 0.05)\\n\\ntrain_acc = history.history['accuracy']\\nvalid_acc = history.history['val_accuracy']\\ntrain_loss = history.history['loss']\\nval_loss = history.history['val_loss']\\n\\n\\n# In[141]:\", metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[141]:\\n\\n\\nfig=plt.figure(figsize=(10,10))\\nfig.add_subplot(2, 1, 1)\\nplt.grid()\\nplt.plot(train_loss, color=\\'blue\\', label=\\'Train\\')\\nplt.plot(val_loss, color=\\'red\\', label=\\'Validation\\')\\nplt.legend()\\nplt.title(\\'Loss Doc2Feature\\', fontsize=20)\\nplt.xticks(fontsize=14)\\nplt.yticks(fontsize=14)\\nplt.tight_layout()\\nfig.savefig(\"Loss_Doc2Feature.png\", dpi=200, bbox_inches=\\'tight\\')', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='fig.add_subplot(2, 1, 2)\\nplt.grid()\\nplt.plot(train_acc, color=\\'blue\\', label=\\'Train\\')\\nplt.plot(valid_acc, color=\\'red\\', label=\\'Validation\\')\\nplt.legend()\\nplt.title(\\'Classification Accuracy Doc2Feature\\', fontsize=20)\\nplt.xticks(fontsize=14)\\nplt.yticks(fontsize=14)\\nplt.tight_layout()\\nfig.savefig(\"Classifier_Accuracy_Doc2Feature.png\", dpi=200, bbox_inches=\\'tight\\')\\n\\n\\n# In[142]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# In[142]:\\n\\n\\nevaluate = model.evaluate(x_test,y_test)\\nprint(\"Test Acuracy is : {:.2f} %\".format(evaluate[1]*100))\\nprint(\"Test Loss is : {:.4f}\".format(evaluate[0]))\\npredictions = model.predict(x_test)\\npredict = []\\nfor i in predictions:\\n    predict.append(np.argmax(i))\\n    \\ncm = confusion_matrix(predict,y_test)\\nacc = accuracy_score(predict,y_test)\\n\\nprint(\"The Confusion matrix is: \\\\n\",cm)\\nprint(acc*100)\\nprint(metrics.classification_report(y_test, predict))\\n\\n\\n# In[ ]:', metadata={'source': 'test_repo\\\\python\\\\Knn_DC.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY=os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":8}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is handle_emojis function in the code?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `handle_emojis` function in the code is used to replace different types of emojis in a tweet with the string 'EMO_POS'. It handles different types of emojis such as smile, laugh, love, wink, and sad emojis. The function uses regular expressions to identify these emojis in the tweet and then replaces them with 'EMO_POS'.\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is word_vector function in the code?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `word_vector` function is used to generate a vector representation of a given list of tokens (words). This is done with the help of the Word2Vec model (`model_w2v`), which has been trained previously. \n",
      "\n",
      "The function works as follows:\n",
      "1. It initializes an empty vector of a given size.\n",
      "2. It then iterates over each word in the tokens. For each word, it tries to find the corresponding vector in the Word2Vec model.\n",
      "3. If the word is in the model's vocabulary, it adds the word's vector to the initially created vector and increments a counter.\n",
      "4. If the word is not in the model's vocabulary (KeyError), it simply skips this word.\n",
      "5. After iterating over all words, if the counter is not zero (i.e., if there were words that were in the model's vocabulary), it divides the vector by the counter. This results in an average vector that represents all the words in the tokens.\n",
      "6. This average vector is then returned. \n",
      "\n",
      "In summary, this function generates an average Word2Vec vector for a given list of words. If a word is not in the model's vocabulary, it is ignored.\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
